{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIATA AUTOMATION DOWNLOAD SYSTEM (TRANSFORM)\n",
    "<b>By: <em>David Serna</em></b><br>\n",
    "<b><em>Data Scientist and Data Engineer</em></b>\n",
    "\n",
    "If you want to know more about the project, what is SIATA and the motivations to make this repository, please [visit](https://github.com/dsernag/siata_automation) it\n",
    "\n",
    "Also, here is my LinkedIn profile: [<img width=\"25px\" src=\"../imgs/linkedIn_PNG32.png\" alt=\"LinkedinLogo\"/>](https://www.linkedin.com/in/dsernag/)\n",
    "\n",
    "This is open source code, so you can use it, but remember to be etic and if is possibly, cite it.Â©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment\n",
    "\n",
    "I highly recommend to create an isolated Python environment, for sanity check, replication purposes and as a learning activity. Also is required to install wget for Windows users.\n",
    "\n",
    "### Conda environment\n",
    "\n",
    "I love [miniconda](https://docs.conda.io/en/latest/miniconda.html), is light and portable and allow an easly manipulation of environments. If you don't install conda, please at least create an independt environment in Python, here is the [documentation](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "After install miniconda, create a environment this way (This command is valid for any operating system):\n",
    "\n",
    "```bash\n",
    "conda create -n siata python=3.9 selenium ipykernel beautifulsoup4 pandas numpy matplotlib seaborn mysql-connector-python\n",
    "```\n",
    "\n",
    "There is also a `requirements.txt` file to install via pip\n",
    "\n",
    "Now we are ready to begin!ðŸš€ðŸš€ðŸš€\n",
    "\n",
    "<b>NOTE:</b> This notebook is not intended no be used in a row. Is intended to be executed line by line, with supervision and understanding the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "CSVFILES = list(map(lambda x: \"../data/air/\" + x ,os.listdir(\"../data/air/\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I've had working with SIATA data, I know in advance that not every csv file had the same schema of others. So, our first step is to check if all datasets have the same column lenght and if so, verify that all have the same column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{36}\n",
      "{'Fecha_Hora__codigoSerial__pm25__calidad_pm25__pm10__calidad_pm10__pm1__calidad_pm1__no__calidad_no__no2__calidad_no2__nox__calidad_nox__ozono__calidad_ozono__co__calidad_co__so2__calidad_so2__pst__calidad_pst__dviento_ssr__calidad_dviento_ssr__haire10_ssr__calidad_haire10_ssr__p_ssr__calidad_p_ssr__pliquida_ssr__calidad_pliquida_ssr__rglobal_ssr__calidad_rglobal_ssr__taire10_ssr__calidad_taire10_ssr__vviento_ssr__calidad_vviento_ssr', 'Unnamed: 0__codigoSerial__pm25__calidad_pm25__pm10__calidad_pm10__pm1__calidad_pm1__no__calidad_no__no2__calidad_no2__nox__calidad_nox__ozono__calidad_ozono__co__calidad_co__so2__calidad_so2__pst__calidad_pst__dviento_ssr__calidad_dviento_ssr__haire10_ssr__calidad_haire10_ssr__p_ssr__calidad_p_ssr__pliquida_ssr__calidad_pliquida_ssr__rglobal_ssr__calidad_rglobal_ssr__taire10_ssr__calidad_taire10_ssr__vviento_ssr__calidad_vviento_ssr'}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Check column lenght and column names\n",
    "\n",
    "# This function check columns in general, length and names\n",
    "def check_columns():\n",
    "    set_column_lenght = set()\n",
    "    set_column_names = set()\n",
    "    for file in CSVFILES:\n",
    "        file_df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "        column_length =  file_df.shape[1]\n",
    "        column_names = file_df.columns.tolist()\n",
    "        set_column_lenght.add(column_length)\n",
    "        set_column_names.add(\"__\".join(column_names))\n",
    "    return set_column_lenght, set_column_names\n",
    "column_lenght, column_names = check_columns()\n",
    "print(column_lenght)\n",
    "print(column_names)\n",
    "print(len(column_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately the whole stand of csv files have 36 columns, however there are 2 patterns of column names. This might be problematic, so let's explore the maladjustment. Aparently the only difference is in the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the pattern create before\n",
    "pattern1 = list(column_names)[0].split(\"__\")\n",
    "pattern2 = list(column_names)[1].split(\"__\")\n",
    "\n",
    "# Drop the first element\n",
    "pattern1.pop(0)\n",
    "pattern2.pop(0)\n",
    "\n",
    "# Rejoin the patterns, and compare it\n",
    "string_pattern1 = \"__\".join(pattern1)\n",
    "string_pattern2 = \"__\".join(pattern2)\n",
    "string_pattern1 == string_pattern2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the supposition  was right. Now let's check how many datafiles have the first column as `Unnamed: 0`\n",
    "\n",
    "As we can observe, is due the fact the first column has no header\n",
    "\n",
    "<img src=\"../imgs/example-csv.PNG\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 669 files with no first column header, this is the 46.23% of the files\n"
     ]
    }
   ],
   "source": [
    "# Check how many csv files have the issue of missing header\n",
    "def check_integrity():\n",
    "    count = 0\n",
    "    for file in CSVFILES:\n",
    "        file_df = pd.read_csv(file, encoding=\"utf-8\")\n",
    "        if file_df.columns[0] == \"Unnamed: 0\":\n",
    "            count += 1\n",
    "    return count\n",
    "count = check_integrity()\n",
    "print(f\"There are {count} files with no first column header, this is the {count*100/len(CSVFILES):.2f}% of the files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we know the issue, let's declare a variable with the columns names. But, before make this process slower than it is, we are going to use the first 3 columns, because we are only interested in `pm25` and `calidad_pm25`. The list of columns will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_columns = ['date', 'station', 'pm25', 'quality_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function will produce a single `pd.DataFrame` object with the whole data. However, to improve performance, we also going to validate the data. How?\n",
    "\n",
    "[Here](../data/Generalidades_Info_Aire.pdf) is the documentation about the air quality stations. This table shows what means `quality_index`:\n",
    "\n",
    "<img src=\"../imgs/data_quality.PNG\" width=\"500px\">\n",
    "\n",
    "So we are going to drop, any record where `quality_index` is -1 or above 2.5, also any row where `pm25` is -9999 and `quality_index` 1. Let's pute in code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and save stage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a single data set filtered\n",
    "def transform():\n",
    "    df_transformed = pd.DataFrame(columns = valid_columns) \n",
    "    for file in CSVFILES:\n",
    "        file_df = pd.read_csv(file, encoding = \"utf-8\", usecols = [i for i in range(0, 4)], names = valid_columns, header = None, skiprows = 1)\n",
    "        conditions = (file_df['quality_index'] == -1) | (file_df['quality_index'] > 2.5) | (file_df['pm25'] < 0) | (file_df['pm25'] == -9999)\n",
    "        file_df_transormed = file_df[~conditions]\n",
    "        df_transformed = pd.concat([df_transformed, file_df_transormed], axis = 0)\n",
    "    df_transformed['date'] = pd.to_datetime(df_transformed['date'])\n",
    "    return df_transformed.reset_index(drop = True)\n",
    "\n",
    "df_transformed = transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station</th>\n",
       "      <th>pm25</th>\n",
       "      <th>quality_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-01 00:00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-01 01:00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-01 02:00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-06-01 03:00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-01 04:00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date station  pm25  quality_index\n",
       "0 2019-06-01 00:00:00      38  12.0            1.0\n",
       "1 2019-06-01 01:00:00      38   9.0            1.0\n",
       "2 2019-06-01 02:00:00      38  10.0            1.0\n",
       "3 2019-06-01 03:00:00      38   6.0            1.0\n",
       "4 2019-06-01 04:00:00      38   1.0            2.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah! We did it!. Let's save our final file in a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.to_csv(\"../data/stage/1605-2208-historic-air.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great folks! It was easy right? Remember that we've made a short preprocessing. Is the job of Data Scientist to do a proper sanity check of the data! Let's put this csv file into a proper RDBMS! [Continue](Load.ipynb) to the Load phase.\n",
    "\n",
    "This is open source code, so you can use it, but remember to be etic and if is possibly, cite it.Â©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('siata')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dd207637e4111fdc2b67fd8821282cea1474323c53f53ddea14f2f8557c2ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
